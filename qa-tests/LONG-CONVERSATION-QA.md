# ğŸ¯ Long Conversation QA Testing Guide

## What This Tests

Your bot will go through **67 message exchanges** across 5 long conversation scenarios:

1. **Event Lifecycle (10 turns)** - Create â†’ Search â†’ Update â†’ Comment â†’ Reminder â†’ Delete
2. **Context Retention (15 turns)** - Tests if bot remembers context across many messages
3. **Multi-Topic Switching (12 turns)** - Rapid topic changes between events, reminders, tasks
4. **Edge Cases (10 turns)** - Ambiguous inputs, corrections, error handling
5. **Stress Test (20 turns)** - Many operations in sequence without degradation

**Total: 67 turns testing logical conversation flow**

---

## ğŸš€ How to Run

### Option 1: Run Locally (Safe - No Production Impact)

```bash
# Run all long conversation tests locally
npm run test:conversations -- long-conversation
```

This will test against your **local development** bot.

---

### Option 2: Run on Production (When You Say!)

```bash
# âš ï¸  THIS SENDS REAL MESSAGES TO PRODUCTION!
./scripts/run-production-qa.sh --production +972XXXXXXXXX
```

**Requirements:**
- âœ… Production bot must be running
- âœ… Phone number must be registered with the bot
- âœ… You must type "YES" to confirm

**What happens:**
1. Sends 67 real WhatsApp messages to production
2. Tests full conversation flows
3. Validates responses
4. Reports pass/fail

---

## ğŸ“Š Test Scenarios Explained

### Scenario 1: Event Lifecycle (10 turns)

```
Turn 1:  Create event with Danny
Turn 2:  Confirm event creation
Turn 3:  Search for the event
Turn 4:  Update location
Turn 5:  Add comment
Turn 6:  View comments
Turn 7:  Add reminder before event
Turn 8:  Check upcoming events
Turn 9:  Update time
Turn 10: Delete event
```

**Tests:**
- âœ… Event creation flow
- âœ… Search accuracy
- âœ… Update operations
- âœ… Comment system
- âœ… Reminder integration
- âœ… Deletion

---

### Scenario 2: Context Retention (15 turns)

```
Turn 1-5:   Build event step-by-step (title â†’ date â†’ time â†’ location)
Turn 6:     Verify event created with all details
Turn 7:     Create different event (context switch)
Turn 8:     Search for first event again
Turn 9:     Create reminder for second event
Turn 10:    List all events
Turn 11-12: Update first event and verify
Turn 13:    Add comment to second event
Turn 14:    List reminders
Turn 15:    Delete second event
```

**Tests:**
- âœ… Multi-step input flow
- âœ… Context preservation across turns
- âœ… Switching between different entities
- âœ… Memory of previous interactions

---

### Scenario 3: Multi-Topic Switching (12 turns)

```
Rapid switching between:
- Events (client meeting)
- Reminders (buy gift)
- Tasks (quarterly report)
```

**Tests:**
- âœ… Topic switch handling
- âœ… No confusion between entities
- âœ… Accurate responses after switches
- âœ… Maintaining state per entity type

---

### Scenario 4: Edge Cases (10 turns)

```
- Ambiguous dates ("Tuesday" - which one?)
- Non-existent searches
- Typos in names (fuzzy matching)
- Non-specific updates
- Past dates (should warn/reject)
- Ambiguous deletes
```

**Tests:**
- âœ… Clarification requests
- âœ… Error handling
- âœ… Fuzzy name matching
- âœ… Date validation
- âœ… Graceful failures

---

### Scenario 5: Stress Test (20 turns)

```
Create 4 events + 2 reminders
Then:
- List all
- Search specific
- Update multiple
- Add comments
- Delete some
- Verify state
```

**Tests:**
- âœ… Performance with multiple items
- âœ… No degradation over 20 turns
- âœ… Accurate state management
- âœ… Correct filtering and search

---

## ğŸ¯ Success Criteria

**Each scenario must:**
- âœ… Complete all turns without errors
- âœ… Respond correctly to each message
- âœ… Maintain context throughout
- âœ… Handle edge cases gracefully
- âœ… End with correct final state

**Overall test passes if:**
- âœ… 5/5 scenarios pass (100%)
- âš ï¸  4/5 scenarios pass (80% - acceptable)
- âŒ <4/5 scenarios pass (FAIL - needs fixes)

---

## ğŸ” What Gets Tested

### Bot Capabilities:
- âœ… NLP classification accuracy over 67 turns
- âœ… Context/state management
- âœ… Memory retention
- âœ… Error handling
- âœ… Response consistency
- âœ… Topic switching
- âœ… Multi-step workflows
- âœ… Edge case handling

### Real-World Scenarios:
- âœ… User changes their mind
- âœ… User provides incomplete info
- âœ… User makes typos
- âœ… User switches topics
- âœ… User goes back to previous topic
- âœ… User asks ambiguous questions

---

## ğŸ“ˆ Expected Results

### Good Bot Performance:
```
Scenario 1: âœ… PASS (10/10 turns)
Scenario 2: âœ… PASS (15/15 turns)
Scenario 3: âœ… PASS (12/12 turns)
Scenario 4: âœ… PASS (10/10 turns - with graceful errors)
Scenario 5: âœ… PASS (20/20 turns)

Overall: 67/67 turns passed (100%)
```

### Acceptable Performance:
```
Scenario 1: âœ… PASS
Scenario 2: âœ… PASS
Scenario 3: âš ï¸  PASS (1-2 minor issues)
Scenario 4: âœ… PASS
Scenario 5: âœ… PASS

Overall: 62-65/67 turns passed (92-97%)
```

### Poor Performance (Needs Fixes):
```
Multiple scenario failures
Context loss between turns
Incorrect responses
Failed edge cases

Overall: <58/67 turns passed (<85%)
```

---

## ğŸ’° Cost to Run

**Production QA Test (67 messages):**
- OpenAI GPT calls: 67 Ã— $0.0002 = **$0.013**
- Gemini calls: 67 Ã— $0.0001 = **$0.007**
- **Total: ~$0.02 per full run**

**Recommendation:** Run once before major deployments

---

## ğŸš¨ Production Testing Checklist

**Before running on production:**

- [ ] Backed up production database
- [ ] Bot is running and responding
- [ ] Test phone number is registered
- [ ] Know how to stop tests if needed
- [ ] Monitoring/logs are accessible
- [ ] Have rollback plan ready

**During testing:**

- [ ] Watch bot responses in real-time
- [ ] Check logs for errors
- [ ] Monitor server CPU/memory
- [ ] Note any unexpected behaviors

**After testing:**

- [ ] Review test results
- [ ] Check database state
- [ ] Clean up test data if needed
- [ ] Document any issues found
- [ ] Update bugs.md if failures

---

## ğŸ› ï¸ Files Created

**Test Scenarios:**
- `botium-tests/convo/long-conversation-01-event-lifecycle.convo.txt`
- `botium-tests/convo/long-conversation-02-context-retention.convo.txt`
- `botium-tests/convo/long-conversation-03-multi-topic.convo.txt`
- `botium-tests/convo/long-conversation-04-edge-cases.convo.txt`
- `botium-tests/convo/long-conversation-05-stress-test.convo.txt`

**Test Runner:**
- `scripts/run-production-qa.sh` - Production test runner

**Documentation:**
- `LONG-CONVERSATION-QA.md` - This guide

---

## ğŸ® Commands Summary

```bash
# Test locally (safe)
npm run test:conversations -- long-conversation

# Test single scenario locally
npm run test:conversations -- long-conversation-01-event-lifecycle.convo.txt

# Test on production (âš ï¸  CAREFUL!)
./scripts/run-production-qa.sh --production +972XXXXXXXXX

# View test files
ls -la botium-tests/convo/long-conversation*.convo.txt
```

---

## ğŸ’¡ Tips

1. **Run locally first** - Iron out issues before production
2. **One scenario at a time** - Debug individual scenarios
3. **Monitor logs** - Watch bot behavior in real-time
4. **Clean test data** - Remove test events/reminders after
5. **Save results** - Screenshot or save test output
6. **Test off-hours** - Less impact on production users

---

## ğŸ¯ When to Run Production QA

**Run full production QA when:**
- âœ… Major NLP changes deployed
- âœ… State management updated
- âœ… Context handling modified
- âœ… Before important demos
- âœ… After bug fixes (regression test)
- âœ… Monthly health check

**DON'T run on production when:**
- âŒ Bot is unstable
- âŒ During peak usage hours
- âŒ Without backup/rollback plan
- âŒ First time (test locally first!)

---

## âœ… You're Ready!

**When you say "run on production", I'll tell you to execute:**

```bash
./scripts/run-production-qa.sh --production +972XXXXXXXXX
```

This will test your bot with **67 logical conversation turns** and show exactly how it performs!

Want to run now, or test locally first?
